
Data Pre-Processing
Import Packages and CSV

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
pd.pandas.set_option("display.max_columns", None)
# Create Dataframe
df = pd.read_csv(r"Visadataset.csv")
# Print shape of dataset
print(df.shape)

(25480, 12)

Data Cleaning
Handling Missing values

    Handling Missing values
    Handling Duplicates
    Check data type
    Understand the dataset

Check Null Values

##these are the features with nan value
features_with_na=[features for features in df.columns if df[features].isnull().sum()>=1]
for feature in features_with_na:
    print(feature,np.round(df[feature].isnull().mean()*100,5), '% missing values')

features_with_na

[]

    There are no null values in the dataset

3.2 Other Data Cleaning steps

Handling Duplicates

df.duplicated().sum()

0

    No Duplicates in the dataset

Remove case_id from the dataset as it cannot used in Model Training

df.drop('case_id', inplace=True, axis=1)

Feature Engineering
Feature Extraction

# importing date class from datetime module
from datetime import date
  
# creating the date object of today's date
todays_date = date.today()
current_year= todays_date.year

current_year

2024

Subtract current year with year of estab to get company's age

df['company_age'] = current_year-df['yr_of_estab']

df

	continent 	education_of_employee 	has_job_experience 	requires_job_training 	no_of_employees 	yr_of_estab 	region_of_employment 	prevailing_wage 	unit_of_wage 	full_time_position 	case_status 	company_age
0 	Asia 	High School 	N 	N 	14513 	2007 	West 	592.2029 	Hour 	Y 	Denied 	17
1 	Asia 	Master's 	Y 	N 	2412 	2002 	Northeast 	83425.6500 	Year 	Y 	Certified 	22
2 	Asia 	Bachelor's 	N 	Y 	44444 	2008 	West 	122996.8600 	Year 	Y 	Denied 	16
3 	Asia 	Bachelor's 	N 	N 	98 	1897 	West 	83434.0300 	Year 	Y 	Denied 	127
4 	Africa 	Master's 	Y 	N 	1082 	2005 	South 	149907.3900 	Year 	Y 	Certified 	19
... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	... 	...
25475 	Asia 	Bachelor's 	Y 	Y 	2601 	2008 	South 	77092.5700 	Year 	Y 	Certified 	16
25476 	Asia 	High School 	Y 	N 	3274 	2006 	Northeast 	279174.7900 	Year 	Y 	Certified 	18
25477 	Asia 	Master's 	Y 	N 	1121 	1910 	South 	146298.8500 	Year 	N 	Certified 	114
25478 	Asia 	Master's 	Y 	Y 	1918 	1887 	West 	86154.7700 	Year 	Y 	Certified 	137
25479 	Asia 	Bachelor's 	Y 	N 	3195 	1960 	Midwest 	70876.9100 	Year 	Y 	Certified 	64

25480 rows × 12 columns

df.drop('yr_of_estab', inplace=True, axis=1)

Type of Features

Numeric Features

num_features = [feature for feature in df.columns if df[feature].dtype != 'O']
print('Num of Numerical Features :', len(num_features))

Num of Numerical Features : 3

Categorical Features

cat_features = [feature for feature in df.columns if df[feature].dtype == 'O']
print('Num of Categorical Features :', len(cat_features))

Num of Categorical Features : 8

Split X and Y

    Split Dataframe to X and y
    Here we set a variable X i.e, independent columns, and a variable y i.e, dependent column as the “Case_Status” column.

X = df.drop('case_status', axis=1)
y = df['case_status']

y.head()

0       Denied
1    Certified
2       Denied
3       Denied
4    Certified
Name: case_status, dtype: object

Manual encoding target column

# If the target column has Denied it is encoded as 1 others as 0
y= np.where(y=='Denied', 1,0)

y

array([1, 0, 1, ..., 0, 0, 0])

Feature Transformation

# distribution of data before scaling
plt.figure(figsize=(12, 6))
for i, col in enumerate(['no_of_employees','prevailing_wage','company_age']):
    plt.subplot(2, 2, i+1)
    sns.histplot(x=X[col], color='indianred')
    plt.xlabel(col)
    plt.tight_layout()

    No of employees and Copmany age column is skewed
    Apply a power transform featurewise to make data more Gaussian-like.

Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.

Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform.

Checking Skewness

What is Skewness ?

    Skewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed. Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution. A normal distribution has a skew of zero

num_features

['no_of_employees', 'prevailing_wage', 'company_age']

# Check Skewness
continuous_features = num_features
X[continuous_features].skew(axis=0, skipna=True)

no_of_employees    12.265260
prevailing_wage     0.755776
company_age         2.037301
dtype: float64

    Positiviely Skewed : company_age, no_of_employees.
    We can handle outliers and then check the skewness.

Apply Power Transformer to Check if it can reduces the outliers

from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method='yeo-johnson')
transform_features = ['company_age', 'no_of_employees']
X_copy = pt.fit_transform(X[transform_features])

X_copy = pd.DataFrame(X_copy, columns=transform_features)

plt.figure(figsize=(12, 5))
for i, col in enumerate(transform_features):
    plt.subplot(1, 2, i+1)
    sns.histplot(x=X_copy[col], color='indianred')
    plt.xlabel(col)
    plt.tight_layout()

Checking Skewness

X_copy.skew(axis=0, skipna=True)

company_age        0.103264
no_of_employees    0.399339
dtype: float64

    Here Yeo-Johnson is used and it supports both positive or negative data for transformation.
    So Power Transformer with yeo-johnson can be used.

for feature in cat_features:
    print(feature,':', df[feature].nunique())

continent : 6
education_of_employee : 4
has_job_experience : 2
requires_job_training : 2
region_of_employment : 5
unit_of_wage : 4
full_time_position : 2
case_status : 2

Feature Encoding and Scaling

One Hot Encoding for Columns which had lesser unique values and not ordinal

    One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.

Ordinal Encoding for Columns which has many unique categories

    Ordinal encoding is used here as label encoder is supported for column transformer.
    Ordinal encoding is used for Ordinal Variable. Variable comprises a finite set of discrete values with a ranked ordering between values.

Standard Scaler

    Standardize features by removing the mean and scaling to unit variance.

Power Transformer

    Power transforms are a technique for transforming numerical input or output variables to have a Gaussian or more-Gaussian-like probability distribution.

Selecting number features for preprocessing

num_features = list(X.select_dtypes(exclude="object").columns)

num_features

['no_of_employees', 'prevailing_wage', 'company_age']

Preprocessing using Column Transformer

# Create Column Transformer with 3 types of transformers
or_columns = ['has_job_experience','requires_job_training','full_time_position','education_of_employee']
oh_columns = ['continent','unit_of_wage','region_of_employment']
transform_columns= ['no_of_employees','company_age']

from sklearn.preprocessing import OneHotEncoder, StandardScaler,OrdinalEncoder, PowerTransformer
from sklearn.compose import ColumnTransformer 
from sklearn.pipeline import Pipeline

numeric_transformer = StandardScaler()
oh_transformer = OneHotEncoder()
ordinal_encoder = OrdinalEncoder()

transform_pipe = Pipeline(steps=[
    ('transformer', PowerTransformer(method='yeo-johnson'))
])

#We are creating this preprocessor object so that we do not have to do all this transformations seprately when we need to do this in future
preprocessor = ColumnTransformer(
    [
        ("OneHotEncoder", oh_transformer, oh_columns),
        ("Ordinal_Encoder", ordinal_encoder, or_columns),
        ("Transformer", transform_pipe, transform_columns),
        ("StandardScaler", numeric_transformer, num_features)
    ]
)

X = preprocessor.fit_transform(X)

X

array([[ 0.        ,  1.        ,  0.        , ...,  0.38666657,
        -1.39853722, -0.65122993],
       [ 0.        ,  1.        ,  0.        , ..., -0.14228155,
         0.1698353 , -0.53321103],
       [ 0.        ,  1.        ,  0.        , ...,  1.69498375,
         0.91907852, -0.6748337 ],
       ...,
       [ 0.        ,  1.        ,  0.        , ..., -0.19871259,
         1.36027953,  1.63833662],
       [ 0.        ,  1.        ,  0.        , ..., -0.16387483,
         0.22150859,  2.18122353],
       [ 0.        ,  1.        ,  0.        , ..., -0.10805575,
        -0.06776315,  0.45814768]])

Classification

from imblearn.combine import SMOTEENN

# Resampling the minority class. The strategy can be changed as required.
smt = SMOTEENN(random_state=42,sampling_strategy='minority' )
# Fit the model to generate the data.
X_res, y_res = smt.fit_resample(X, y)

Train Test Split

    The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.

    It is a fast and easy procedure to perform, the results of which allow you to compare the performance of machine learning algorithms.

from sklearn.model_selection import  train_test_split
# separate dataset into train and test
X_train, X_test, y_train, y_test = train_test_split(X_res,y_res,test_size=0.2,random_state=42)
X_train.shape, X_test.shape

((13638, 24), (3410, 24))

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay,precision_score, recall_score,f1_score, roc_auc_score,roc_curve 
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

def evaluate_clf(true, predicted):
    acc = accuracy_score(true, predicted) # Calculate Accuracy
    f1 = f1_score(true, predicted) # Calculate F1-score
    precision = precision_score(true, predicted) # Calculate Precision
    recall = recall_score(true, predicted)  # Calculate Recall
    roc_auc = roc_auc_score(true, predicted) #Calculate Roc
    return acc, f1 , precision, recall, roc_auc

models = {
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Logistic Regression": LogisticRegression(),
     "K-Neighbors Classifier": KNeighborsClassifier(),
    "XGBClassifier": XGBClassifier(), 
     "CatBoosting Classifier": CatBoostClassifier(verbose=False),
     "Support Vector Classifier": SVC(),
    "AdaBoost Classifier": AdaBoostClassifier()
}

# Create a function which can evaluate models and return a report 
def evaluate_models(X, y, models):
    '''
    This function takes in X and y and models dictionary as input
    It splits the data into Train Test split
    Iterates through the given model dictionary and evaluates the metrics
    Returns: Dataframe which contains report of all models metrics with cost
    '''
    # separate dataset into train and test
    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)
    
    models_list = []
    accuracy_list = []
    auc= []
    
    for i in range(len(list(models))):
        model = list(models.values())[i]
        model.fit(X_train, y_train) # Train model

        # Make predictions
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        # Training set performance
        model_train_accuracy, model_train_f1,model_train_precision,\
        model_train_recall,model_train_rocauc_score=evaluate_clf(y_train ,y_train_pred)


        # Test set performance
        model_test_accuracy,model_test_f1,model_test_precision,\
        model_test_recall,model_test_rocauc_score=evaluate_clf(y_test, y_test_pred)

        print(list(models.keys())[i])
        models_list.append(list(models.keys())[i])

        print('Model performance for Training set')
        print("- Accuracy: {:.4f}".format(model_train_accuracy))
        print('- F1 score: {:.4f}'.format(model_train_f1)) 
        print('- Precision: {:.4f}'.format(model_train_precision))
        print('- Recall: {:.4f}'.format(model_train_recall))
        print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))

        print('----------------------------------')

        print('Model performance for Test set')
        print('- Accuracy: {:.4f}'.format(model_test_accuracy))
        accuracy_list.append(model_test_accuracy)
        print('- F1 score: {:.4f}'.format(model_test_f1))
        print('- Precision: {:.4f}'.format(model_test_precision))
        print('- Recall: {:.4f}'.format(model_test_recall))
        print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))
        auc.append(model_test_rocauc_score)
        print('='*35)
        print('\n')
        
    report=pd.DataFrame(list(zip(models_list, accuracy_list)), columns=['Model Name', 'Accuracy']).sort_values(by=['Accuracy'], ascending=False)
        
    return report

Model Training

base_model_report =evaluate_models(X=X_res, y=y_res, models=models)

Random Forest
Model performance for Training set
- Accuracy: 1.0000
- F1 score: 1.0000
- Precision: 1.0000
- Recall: 1.0000
- Roc Auc Score: 1.0000
----------------------------------
Model performance for Test set
- Accuracy: 0.9548
- F1 score: 0.9584
- Precision: 0.9569
- Recall: 0.9600
- Roc Auc Score: 0.9544
===================================


Decision Tree
Model performance for Training set
- Accuracy: 1.0000
- F1 score: 1.0000
- Precision: 1.0000
- Recall: 1.0000
- Roc Auc Score: 1.0000
----------------------------------
Model performance for Test set
- Accuracy: 0.9284
- F1 score: 0.9343
- Precision: 0.9298
- Recall: 0.9389
- Roc Auc Score: 0.9275
===================================


Gradient Boosting
Model performance for Training set
- Accuracy: 0.8822
- F1 score: 0.8903
- Precision: 0.8945
- Recall: 0.8862
- Roc Auc Score: 0.8819
----------------------------------
Model performance for Test set
- Accuracy: 0.8956
- F1 score: 0.9031
- Precision: 0.9095
- Recall: 0.8967
- Roc Auc Score: 0.8955
===================================


Logistic Regression
Model performance for Training set
- Accuracy: 0.7348
- F1 score: 0.7499
- Precision: 0.7628
- Recall: 0.7376
- Roc Auc Score: 0.7345
----------------------------------
Model performance for Test set
- Accuracy: 0.7452
- F1 score: 0.7592
- Precision: 0.7784
- Recall: 0.7409
- Roc Auc Score: 0.7456
===================================


K-Neighbors Classifier
Model performance for Training set
- Accuracy: 0.9636
- F1 score: 0.9667
- Precision: 0.9553
- Recall: 0.9784
- Roc Auc Score: 0.9624
----------------------------------
Model performance for Test set
- Accuracy: 0.9402
- F1 score: 0.9457
- Precision: 0.9318
- Recall: 0.9600
- Roc Auc Score: 0.9383
===================================


XGBClassifier
Model performance for Training set
- Accuracy: 0.9809
- F1 score: 0.9823
- Precision: 0.9825
- Recall: 0.9821
- Roc Auc Score: 0.9808
----------------------------------
Model performance for Test set
- Accuracy: 0.9352
- F1 score: 0.9400
- Precision: 0.9443
- Recall: 0.9356
- Roc Auc Score: 0.9351
===================================


CatBoosting Classifier
Model performance for Training set
- Accuracy: 0.9527
- F1 score: 0.9559
- Precision: 0.9614
- Recall: 0.9505
- Roc Auc Score: 0.9529
----------------------------------
Model performance for Test set
- Accuracy: 0.9296
- F1 score: 0.9342
- Precision: 0.9472
- Recall: 0.9216
- Roc Auc Score: 0.9304
===================================


Support Vector Classifier
Model performance for Training set
- Accuracy: 0.8716
- F1 score: 0.8808
- Precision: 0.8818
- Recall: 0.8798
- Roc Auc Score: 0.8709
----------------------------------
Model performance for Test set
- Accuracy: 0.8701
- F1 score: 0.8802
- Precision: 0.8800
- Recall: 0.8805
- Roc Auc Score: 0.8691
===================================


AdaBoost Classifier
Model performance for Training set
- Accuracy: 0.8539
- F1 score: 0.8636
- Precision: 0.8696
- Recall: 0.8576
- Roc Auc Score: 0.8535
----------------------------------
Model performance for Test set
- Accuracy: 0.8683
- F1 score: 0.8774
- Precision: 0.8863
- Recall: 0.8686
- Roc Auc Score: 0.8683
===================================


Results of All Models

base_model_report

	Model Name 	Accuracy
0 	Random Forest 	0.954839
4 	K-Neighbors Classifier 	0.940176
5 	XGBClassifier 	0.935191
6 	CatBoosting Classifier 	0.929619
1 	Decision Tree 	0.928446
2 	Gradient Boosting 	0.895601
7 	Support Vector Classifier 	0.870088
8 	AdaBoost Classifier 	0.868328
3 	Logistic Regression 	0.745161

Here we can use Random Forest for Hyper Parameter Tuning

Define the parameter distribution for Random forest

#Initialize few parameter for Hyperparamter tuning
xgboost_params = {
    'max_depth':range(3,10,2),
    'min_child_weight':range(1,6,2)
}

rf_params = {
    "max_depth": [10, 12, None, 15, 20],
    "max_features": ['sqrt', 'log2', None],
    "n_estimators": [10, 50, 100, 200]
}

knn_params = {
    "algorithm": ['auto', 'ball_tree', 'kd_tree','brute'],
    "weights": ['uniform', 'distance'],
    "n_neighbors": [3, 4, 5, 7, 9],
}

# Models list for Hyperparameter tuning
randomcv_models = [
    ('XGBoost', XGBClassifier(), xgboost_params),
    ("RF", RandomForestClassifier(), rf_params),
    ("KNN", KNeighborsClassifier(), knn_params)
]

Create a function for model training and report which can be used in hyperparameter tuning loop

from sklearn.model_selection import RandomizedSearchCV

model_param = {}
for name, model, params in randomcv_models:
    random = RandomizedSearchCV(estimator=model,
                                   param_distributions=params,
                                   n_iter=100,
                                   cv=3,
                                   verbose=2,
                                   n_jobs=-1)
    random.fit(X_res, y_res)
    model_param[name] = random.best_params_

for model_name in model_param:
    print(f"---------------- Best Params for {model_name} -------------------")
    print(model_param[model_name])

Fitting 3 folds for each of 12 candidates, totalling 36 fits
[CV] END ....................max_depth=3, min_child_weight=1; total time=   0.3s
[CV] END ....................max_depth=3, min_child_weight=1; total time=   0.3s
[CV] END ....................max_depth=3, min_child_weight=1; total time=   0.4s
[CV] END ....................max_depth=3, min_child_weight=3; total time=   0.5s
[CV] END ....................max_depth=3, min_child_weight=3; total time=   0.5s
[CV] END ....................max_depth=3, min_child_weight=3; total time=   0.5s
[CV] END ....................max_depth=3, min_child_weight=5; total time=   0.5s
[CV] END ....................max_depth=3, min_child_weight=5; total time=   0.4s
[CV] END ....................max_depth=3, min_child_weight=5; total time=   0.3s
[CV] END ....................max_depth=5, min_child_weight=1; total time=   0.6s
[CV] END ....................max_depth=5, min_child_weight=1; total time=   0.6s
[CV] END ....................max_depth=5, min_child_weight=1; total time=   0.6s
[CV] END ....................max_depth=5, min_child_weight=3; total time=   0.6s
[CV] END ....................max_depth=5, min_child_weight=3; total time=   0.5s
[CV] END ....................max_depth=5, min_child_weight=3; total time=   0.5s
[CV] END ....................max_depth=5, min_child_weight=5; total time=   0.6s
[CV] END ....................max_depth=5, min_child_weight=5; total time=   0.7s
[CV] END ....................max_depth=5, min_child_weight=5; total time=   1.0s
[CV] END ....................max_depth=7, min_child_weight=1; total time=   1.4s
[CV] END ....................max_depth=7, min_child_weight=1; total time=   1.4s
[CV] END ....................max_depth=7, min_child_weight=1; total time=   1.1s
[CV] END ....................max_depth=7, min_child_weight=3; total time=   0.8s
[CV] END ....................max_depth=7, min_child_weight=3; total time=   0.8s
[CV] END ....................max_depth=7, min_child_weight=3; total time=   0.8s
[CV] END ....................max_depth=7, min_child_weight=5; total time=   0.8s
[CV] END ....................max_depth=7, min_child_weight=5; total time=   0.7s
[CV] END ....................max_depth=7, min_child_weight=5; total time=   0.7s
[CV] END ....................max_depth=9, min_child_weight=1; total time=   1.5s
[CV] END ....................max_depth=9, min_child_weight=1; total time=   1.6s
[CV] END ....................max_depth=9, min_child_weight=1; total time=   1.4s
[CV] END ....................max_depth=9, min_child_weight=3; total time=   1.2s
[CV] END ....................max_depth=9, min_child_weight=3; total time=   1.0s
[CV] END ....................max_depth=9, min_child_weight=3; total time=   1.0s
[CV] END ....................max_depth=9, min_child_weight=5; total time=   1.0s
[CV] END ....................max_depth=9, min_child_weight=5; total time=   0.8s
[CV] END ....................max_depth=9, min_child_weight=5; total time=   0.4s
Fitting 3 folds for each of 60 candidates, totalling 180 fits
[CV] END ...max_depth=10, max_features=sqrt, n_estimators=10; total time=   0.2s
[CV] END ...max_depth=10, max_features=sqrt, n_estimators=10; total time=   0.2s
[CV] END ...max_depth=10, max_features=sqrt, n_estimators=10; total time=   0.2s
[CV] END ...max_depth=10, max_features=sqrt, n_estimators=50; total time=   1.2s
[CV] END ...max_depth=10, max_features=sqrt, n_estimators=50; total time=   1.1s
[CV] END ...max_depth=10, max_features=sqrt, n_estimators=50; total time=   1.1s
[CV] END ..max_depth=10, max_features=sqrt, n_estimators=100; total time=   2.4s
[CV] END ..max_depth=10, max_features=sqrt, n_estimators=100; total time=   2.4s
[CV] END ..max_depth=10, max_features=sqrt, n_estimators=100; total time=   2.3s
[CV] END ...max_depth=10, max_features=log2, n_estimators=10; total time=   0.3s
[CV] END ...max_depth=10, max_features=log2, n_estimators=10; total time=   0.2s
[CV] END ...max_depth=10, max_features=log2, n_estimators=10; total time=   0.2s
[CV] END ...max_depth=10, max_features=log2, n_estimators=50; total time=   2.8s
[CV] END ..max_depth=10, max_features=sqrt, n_estimators=200; total time=   6.9s
[CV] END ...max_depth=10, max_features=log2, n_estimators=50; total time=   1.8s
[CV] END ..max_depth=10, max_features=sqrt, n_estimators=200; total time=   7.1s
[CV] END ...max_depth=10, max_features=log2, n_estimators=50; total time=   1.6s
[CV] END ..max_depth=10, max_features=sqrt, n_estimators=200; total time=   7.4s
[CV] END ..max_depth=10, max_features=log2, n_estimators=100; total time=   3.5s
[CV] END ..max_depth=10, max_features=log2, n_estimators=100; total time=   3.4s
[CV] END ..max_depth=10, max_features=log2, n_estimators=100; total time=   3.4s
[CV] END ...max_depth=10, max_features=None, n_estimators=10; total time=   1.3s
[CV] END ...max_depth=10, max_features=None, n_estimators=10; total time=   1.3s
[CV] END ...max_depth=10, max_features=None, n_estimators=10; total time=   1.4s
[CV] END ..max_depth=10, max_features=log2, n_estimators=200; total time=   7.0s
[CV] END ..max_depth=10, max_features=log2, n_estimators=200; total time=   6.6s
[CV] END ..max_depth=10, max_features=log2, n_estimators=200; total time=   6.7s
[CV] END ...max_depth=10, max_features=None, n_estimators=50; total time=   8.3s
[CV] END ...max_depth=10, max_features=None, n_estimators=50; total time=   7.8s
[CV] END ...max_depth=10, max_features=None, n_estimators=50; total time=   7.5s
[CV] END ..max_depth=10, max_features=None, n_estimators=100; total time=  18.2s
[CV] END ..max_depth=10, max_features=None, n_estimators=100; total time=  15.1s
[CV] END ..max_depth=10, max_features=None, n_estimators=100; total time=  15.7s
[CV] END ...max_depth=12, max_features=sqrt, n_estimators=10; total time=   0.3s
[CV] END ...max_depth=12, max_features=sqrt, n_estimators=10; total time=   0.3s
[CV] END ...max_depth=12, max_features=sqrt, n_estimators=10; total time=   0.3s
[CV] END ...max_depth=12, max_features=sqrt, n_estimators=50; total time=   1.6s
[CV] END ...max_depth=12, max_features=sqrt, n_estimators=50; total time=   1.5s
[CV] END ...max_depth=12, max_features=sqrt, n_estimators=50; total time=   1.5s
[CV] END ..max_depth=12, max_features=sqrt, n_estimators=100; total time=   3.1s
[CV] END ..max_depth=12, max_features=sqrt, n_estimators=100; total time=   3.1s
[CV] END ..max_depth=10, max_features=None, n_estimators=200; total time=  28.2s
[CV] END ..max_depth=12, max_features=sqrt, n_estimators=100; total time=   3.5s
[CV] END ..max_depth=10, max_features=None, n_estimators=200; total time=  22.6s
[CV] END ..max_depth=12, max_features=sqrt, n_estimators=200; total time=   6.6s
[CV] END ...max_depth=12, max_features=log2, n_estimators=10; total time=   0.3s
[CV] END ...max_depth=12, max_features=log2, n_estimators=10; total time=   0.3s
[CV] END ..max_depth=10, max_features=None, n_estimators=200; total time=  21.4s
[CV] END ...max_depth=12, max_features=log2, n_estimators=10; total time=   0.4s
[CV] END ..max_depth=12, max_features=sqrt, n_estimators=200; total time=   6.4s
[CV] END ...max_depth=12, max_features=log2, n_estimators=50; total time=   2.1s
[CV] END ...max_depth=12, max_features=log2, n_estimators=50; total time=   1.9s
[CV] END ...max_depth=12, max_features=log2, n_estimators=50; total time=   1.9s
[CV] END ..max_depth=12, max_features=sqrt, n_estimators=200; total time=   6.8s
[CV] END ..max_depth=12, max_features=log2, n_estimators=100; total time=   3.9s
[CV] END ..max_depth=12, max_features=log2, n_estimators=100; total time=   4.1s
[CV] END ..max_depth=12, max_features=log2, n_estimators=100; total time=   3.8s
[CV] END ...max_depth=12, max_features=None, n_estimators=10; total time=   1.3s
[CV] END ...max_depth=12, max_features=None, n_estimators=10; total time=   1.1s
[CV] END ...max_depth=12, max_features=None, n_estimators=10; total time=   1.1s
[CV] END ..max_depth=12, max_features=log2, n_estimators=200; total time=   6.5s
[CV] END ..max_depth=12, max_features=log2, n_estimators=200; total time=   5.7s
[CV] END ..max_depth=12, max_features=log2, n_estimators=200; total time=   6.2s
[CV] END ...max_depth=12, max_features=None, n_estimators=50; total time=   7.5s
[CV] END ...max_depth=12, max_features=None, n_estimators=50; total time=   6.6s
[CV] END ...max_depth=12, max_features=None, n_estimators=50; total time=   6.9s
[CV] END ..max_depth=12, max_features=None, n_estimators=100; total time=  12.1s
[CV] END ..max_depth=12, max_features=None, n_estimators=100; total time=  10.4s
[CV] END ..max_depth=12, max_features=None, n_estimators=100; total time=   9.5s
[CV] END .max_depth=None, max_features=sqrt, n_estimators=10; total time=   0.4s
[CV] END .max_depth=None, max_features=sqrt, n_estimators=10; total time=   0.3s
[CV] END .max_depth=None, max_features=sqrt, n_estimators=10; total time=   0.3s
[CV] END .max_depth=None, max_features=sqrt, n_estimators=50; total time=   1.7s
[CV] END .max_depth=None, max_features=sqrt, n_estimators=50; total time=   1.6s
[CV] END .max_depth=None, max_features=sqrt, n_estimators=50; total time=   2.0s
[CV] END max_depth=None, max_features=sqrt, n_estimators=100; total time=   3.2s
[CV] END ..max_depth=12, max_features=None, n_estimators=200; total time=  22.3s
[CV] END max_depth=None, max_features=sqrt, n_estimators=100; total time=   3.3s
[CV] END max_depth=None, max_features=sqrt, n_estimators=100; total time=   3.1s
[CV] END ..max_depth=12, max_features=None, n_estimators=200; total time=  21.4s
[CV] END max_depth=None, max_features=sqrt, n_estimators=200; total time=   6.7s
[CV] END ..max_depth=12, max_features=None, n_estimators=200; total time=  20.0s
[CV] END .max_depth=None, max_features=log2, n_estimators=10; total time=   0.3s
[CV] END .max_depth=None, max_features=log2, n_estimators=10; total time=   0.3s
[CV] END .max_depth=None, max_features=log2, n_estimators=10; total time=   0.3s
[CV] END .max_depth=None, max_features=log2, n_estimators=50; total time=   1.6s
[CV] END .max_depth=None, max_features=log2, n_estimators=50; total time=   1.6s
[CV] END max_depth=None, max_features=sqrt, n_estimators=200; total time=   6.4s
[CV] END .max_depth=None, max_features=log2, n_estimators=50; total time=   1.8s
[CV] END max_depth=None, max_features=sqrt, n_estimators=200; total time=   6.5s
[CV] END max_depth=None, max_features=log2, n_estimators=100; total time=   3.4s
[CV] END max_depth=None, max_features=log2, n_estimators=100; total time=   3.4s
[CV] END max_depth=None, max_features=log2, n_estimators=100; total time=   3.3s
[CV] END .max_depth=None, max_features=None, n_estimators=10; total time=   1.3s
[CV] END .max_depth=None, max_features=None, n_estimators=10; total time=   1.2s
[CV] END .max_depth=None, max_features=None, n_estimators=10; total time=   1.1s
[CV] END max_depth=None, max_features=log2, n_estimators=200; total time=   6.8s
[CV] END max_depth=None, max_features=log2, n_estimators=200; total time=   6.4s
[CV] END max_depth=None, max_features=log2, n_estimators=200; total time=   6.6s
[CV] END .max_depth=None, max_features=None, n_estimators=50; total time=   6.8s
[CV] END .max_depth=None, max_features=None, n_estimators=50; total time=   6.6s
[CV] END .max_depth=None, max_features=None, n_estimators=50; total time=   6.4s
[CV] END max_depth=None, max_features=None, n_estimators=100; total time=  15.7s
[CV] END max_depth=None, max_features=None, n_estimators=100; total time=  14.0s
[CV] END max_depth=None, max_features=None, n_estimators=100; total time=  15.2s
[CV] END ...max_depth=15, max_features=sqrt, n_estimators=10; total time=   0.4s
[CV] END ...max_depth=15, max_features=sqrt, n_estimators=10; total time=   0.4s
[CV] END ...max_depth=15, max_features=sqrt, n_estimators=10; total time=   0.4s
[CV] END ...max_depth=15, max_features=sqrt, n_estimators=50; total time=   1.7s
[CV] END ...max_depth=15, max_features=sqrt, n_estimators=50; total time=   1.4s
[CV] END ...max_depth=15, max_features=sqrt, n_estimators=50; total time=   2.2s
[CV] END ..max_depth=15, max_features=sqrt, n_estimators=100; total time=   4.2s
[CV] END ..max_depth=15, max_features=sqrt, n_estimators=100; total time=   3.6s
[CV] END max_depth=None, max_features=None, n_estimators=200; total time=  31.7s
[CV] END ..max_depth=15, max_features=sqrt, n_estimators=100; total time=   3.8s
[CV] END max_depth=None, max_features=None, n_estimators=200; total time=  32.9s
[CV] END ..max_depth=15, max_features=sqrt, n_estimators=200; total time=  10.6s
[CV] END ..max_depth=15, max_features=sqrt, n_estimators=200; total time=  10.4s
[CV] END ...max_depth=15, max_features=log2, n_estimators=10; total time=   0.5s
[CV] END ...max_depth=15, max_features=log2, n_estimators=10; total time=   0.5s
[CV] END ...max_depth=15, max_features=log2, n_estimators=10; total time=   0.5s
[CV] END max_depth=None, max_features=None, n_estimators=200; total time=  30.8s
[CV] END ...max_depth=15, max_features=log2, n_estimators=50; total time=   2.8s
[CV] END ...max_depth=15, max_features=log2, n_estimators=50; total time=   2.8s
[CV] END ...max_depth=15, max_features=log2, n_estimators=50; total time=   2.8s
[CV] END ..max_depth=15, max_features=log2, n_estimators=100; total time=   6.0s
[CV] END ..max_depth=15, max_features=log2, n_estimators=100; total time=   6.4s
[CV] END ..max_depth=15, max_features=log2, n_estimators=100; total time=   6.3s
[CV] END ..max_depth=15, max_features=sqrt, n_estimators=200; total time=  11.4s
[CV] END ...max_depth=15, max_features=None, n_estimators=10; total time=   2.0s
[CV] END ...max_depth=15, max_features=None, n_estimators=10; total time=   2.1s
[CV] END ...max_depth=15, max_features=None, n_estimators=10; total time=   2.1s
[CV] END ..max_depth=15, max_features=log2, n_estimators=200; total time=  11.0s
[CV] END ..max_depth=15, max_features=log2, n_estimators=200; total time=  11.5s
[CV] END ..max_depth=15, max_features=log2, n_estimators=200; total time=  10.7s
[CV] END ...max_depth=15, max_features=None, n_estimators=50; total time=  11.5s
[CV] END ...max_depth=15, max_features=None, n_estimators=50; total time=   9.8s
[CV] END ...max_depth=15, max_features=None, n_estimators=50; total time=  10.9s
[CV] END ..max_depth=15, max_features=None, n_estimators=100; total time=  23.1s
[CV] END ..max_depth=15, max_features=None, n_estimators=100; total time=  20.8s
[CV] END ..max_depth=15, max_features=None, n_estimators=100; total time=  18.5s
[CV] END ...max_depth=20, max_features=sqrt, n_estimators=10; total time=   0.5s
[CV] END ...max_depth=20, max_features=sqrt, n_estimators=10; total time=   0.4s
[CV] END ...max_depth=20, max_features=sqrt, n_estimators=10; total time=   0.3s
[CV] END ...max_depth=20, max_features=sqrt, n_estimators=50; total time=   2.0s
[CV] END ...max_depth=20, max_features=sqrt, n_estimators=50; total time=   1.8s
[CV] END ...max_depth=20, max_features=sqrt, n_estimators=50; total time=   1.5s
[CV] END ..max_depth=20, max_features=sqrt, n_estimators=100; total time=   3.3s
[CV] END ..max_depth=20, max_features=sqrt, n_estimators=100; total time=   3.4s
[CV] END ..max_depth=15, max_features=None, n_estimators=200; total time=  32.9s
[CV] END ..max_depth=20, max_features=sqrt, n_estimators=100; total time=   3.0s
[CV] END ..max_depth=15, max_features=None, n_estimators=200; total time=  26.2s
[CV] END ..max_depth=20, max_features=sqrt, n_estimators=200; total time=   7.4s
[CV] END ..max_depth=15, max_features=None, n_estimators=200; total time=  22.8s
[CV] END ...max_depth=20, max_features=log2, n_estimators=10; total time=   0.3s
[CV] END ...max_depth=20, max_features=log2, n_estimators=10; total time=   0.3s
[CV] END ...max_depth=20, max_features=log2, n_estimators=10; total time=   0.3s
[CV] END ..max_depth=20, max_features=sqrt, n_estimators=200; total time=   7.1s
[CV] END ...max_depth=20, max_features=log2, n_estimators=50; total time=   1.7s
[CV] END ...max_depth=20, max_features=log2, n_estimators=50; total time=   1.6s
[CV] END ...max_depth=20, max_features=log2, n_estimators=50; total time=   1.6s
[CV] END ..max_depth=20, max_features=sqrt, n_estimators=200; total time=   6.8s
[CV] END ..max_depth=20, max_features=log2, n_estimators=100; total time=   3.8s
[CV] END ..max_depth=20, max_features=log2, n_estimators=100; total time=   3.6s
[CV] END ..max_depth=20, max_features=log2, n_estimators=100; total time=   3.5s
[CV] END ...max_depth=20, max_features=None, n_estimators=10; total time=   1.2s
[CV] END ...max_depth=20, max_features=None, n_estimators=10; total time=   1.3s
[CV] END ...max_depth=20, max_features=None, n_estimators=10; total time=   2.0s
[CV] END ..max_depth=20, max_features=log2, n_estimators=200; total time=   9.1s
[CV] END ..max_depth=20, max_features=log2, n_estimators=200; total time=   8.7s
[CV] END ..max_depth=20, max_features=log2, n_estimators=200; total time=   8.9s
[CV] END ...max_depth=20, max_features=None, n_estimators=50; total time=   9.0s
[CV] END ...max_depth=20, max_features=None, n_estimators=50; total time=   7.5s
[CV] END ...max_depth=20, max_features=None, n_estimators=50; total time=   6.9s
[CV] END ..max_depth=20, max_features=None, n_estimators=100; total time=  16.0s
[CV] END ..max_depth=20, max_features=None, n_estimators=100; total time=  15.3s
[CV] END ..max_depth=20, max_features=None, n_estimators=100; total time=  14.1s
[CV] END ..max_depth=20, max_features=None, n_estimators=200; total time=  27.9s
[CV] END ..max_depth=20, max_features=None, n_estimators=200; total time=  22.4s
[CV] END ..max_depth=20, max_features=None, n_estimators=200; total time=  18.9s
Fitting 3 folds for each of 40 candidates, totalling 120 fits
[CV] END .....algorithm=auto, n_neighbors=3, weights=uniform; total time=   1.3s
[CV] END .....algorithm=auto, n_neighbors=3, weights=uniform; total time=   1.3s
[CV] END .....algorithm=auto, n_neighbors=3, weights=uniform; total time=   1.3s
[CV] END ....algorithm=auto, n_neighbors=3, weights=distance; total time=   1.3s
[CV] END ....algorithm=auto, n_neighbors=3, weights=distance; total time=   1.2s
[CV] END ....algorithm=auto, n_neighbors=3, weights=distance; total time=   1.2s
[CV] END .....algorithm=auto, n_neighbors=4, weights=uniform; total time=   1.2s
[CV] END .....algorithm=auto, n_neighbors=4, weights=uniform; total time=   1.2s
[CV] END .....algorithm=auto, n_neighbors=4, weights=uniform; total time=   1.2s
[CV] END ....algorithm=auto, n_neighbors=4, weights=distance; total time=   1.2s
[CV] END ....algorithm=auto, n_neighbors=4, weights=distance; total time=   1.2s
[CV] END ....algorithm=auto, n_neighbors=4, weights=distance; total time=   1.2s
[CV] END .....algorithm=auto, n_neighbors=5, weights=uniform; total time=   1.1s
[CV] END .....algorithm=auto, n_neighbors=5, weights=uniform; total time=   1.1s
[CV] END .....algorithm=auto, n_neighbors=5, weights=uniform; total time=   1.1s
[CV] END ....algorithm=auto, n_neighbors=5, weights=distance; total time=   1.1s
[CV] END ....algorithm=auto, n_neighbors=5, weights=distance; total time=   1.2s
[CV] END ....algorithm=auto, n_neighbors=5, weights=distance; total time=   1.1s
[CV] END .....algorithm=auto, n_neighbors=7, weights=uniform; total time=   1.1s
[CV] END .....algorithm=auto, n_neighbors=7, weights=uniform; total time=   1.1s
[CV] END .....algorithm=auto, n_neighbors=7, weights=uniform; total time=   1.1s
[CV] END ....algorithm=auto, n_neighbors=7, weights=distance; total time=   1.1s
[CV] END ....algorithm=auto, n_neighbors=7, weights=distance; total time=   1.1s
[CV] END ....algorithm=auto, n_neighbors=7, weights=distance; total time=   1.1s
[CV] END .....algorithm=auto, n_neighbors=9, weights=uniform; total time=   1.0s
[CV] END .....algorithm=auto, n_neighbors=9, weights=uniform; total time=   1.0s
[CV] END ....algorithm=auto, n_neighbors=9, weights=distance; total time=   1.0s
[CV] END .....algorithm=auto, n_neighbors=9, weights=uniform; total time=   1.0s
[CV] END ....algorithm=auto, n_neighbors=9, weights=distance; total time=   1.0s
[CV] END ....algorithm=auto, n_neighbors=9, weights=distance; total time=   1.1s
[CV] END algorithm=ball_tree, n_neighbors=3, weights=uniform; total time=   7.8s
[CV] END algorithm=ball_tree, n_neighbors=3, weights=uniform; total time=   7.9s
[CV] END algorithm=ball_tree, n_neighbors=3, weights=distance; total time=   7.0s
[CV] END algorithm=ball_tree, n_neighbors=3, weights=uniform; total time=   7.7s
[CV] END algorithm=ball_tree, n_neighbors=3, weights=distance; total time=   7.0s
[CV] END algorithm=ball_tree, n_neighbors=3, weights=distance; total time=   7.1s
[CV] END algorithm=ball_tree, n_neighbors=4, weights=uniform; total time=   8.6s
[CV] END algorithm=ball_tree, n_neighbors=4, weights=uniform; total time=   8.8s
[CV] END algorithm=ball_tree, n_neighbors=4, weights=distance; total time=   9.7s
[CV] END algorithm=ball_tree, n_neighbors=4, weights=uniform; total time=  10.4s
[CV] END algorithm=ball_tree, n_neighbors=4, weights=distance; total time=   9.3s
[CV] END algorithm=ball_tree, n_neighbors=4, weights=distance; total time=   9.4s
[CV] END algorithm=ball_tree, n_neighbors=5, weights=uniform; total time=   9.5s
[CV] END algorithm=ball_tree, n_neighbors=5, weights=uniform; total time=   9.6s
[CV] END algorithm=ball_tree, n_neighbors=5, weights=uniform; total time=   9.2s
[CV] END algorithm=ball_tree, n_neighbors=5, weights=distance; total time=   8.3s
[CV] END algorithm=ball_tree, n_neighbors=5, weights=distance; total time=  11.3s
[CV] END algorithm=ball_tree, n_neighbors=5, weights=distance; total time=  12.1s
[CV] END algorithm=ball_tree, n_neighbors=7, weights=uniform; total time=  14.0s
[CV] END algorithm=ball_tree, n_neighbors=7, weights=uniform; total time=  14.0s
[CV] END algorithm=ball_tree, n_neighbors=7, weights=distance; total time=  12.7s
[CV] END algorithm=ball_tree, n_neighbors=7, weights=uniform; total time=  14.4s
[CV] END algorithm=ball_tree, n_neighbors=7, weights=distance; total time=  13.3s
[CV] END algorithm=ball_tree, n_neighbors=7, weights=distance; total time=  13.5s
[CV] END algorithm=ball_tree, n_neighbors=9, weights=uniform; total time=  14.9s
[CV] END algorithm=ball_tree, n_neighbors=9, weights=uniform; total time=  15.1s
[CV] END algorithm=ball_tree, n_neighbors=9, weights=distance; total time=  13.7s
[CV] END algorithm=ball_tree, n_neighbors=9, weights=uniform; total time=  14.7s
[CV] END ..algorithm=kd_tree, n_neighbors=3, weights=uniform; total time=   3.4s
[CV] END ..algorithm=kd_tree, n_neighbors=3, weights=uniform; total time=   3.5s
[CV] END .algorithm=kd_tree, n_neighbors=3, weights=distance; total time=   3.0s
[CV] END ..algorithm=kd_tree, n_neighbors=3, weights=uniform; total time=   3.9s
[CV] END algorithm=ball_tree, n_neighbors=9, weights=distance; total time=  12.0s
[CV] END algorithm=ball_tree, n_neighbors=9, weights=distance; total time=  11.9s
[CV] END .algorithm=kd_tree, n_neighbors=3, weights=distance; total time=   3.3s
[CV] END .algorithm=kd_tree, n_neighbors=3, weights=distance; total time=   3.3s
[CV] END .algorithm=kd_tree, n_neighbors=4, weights=distance; total time=   3.0s
[CV] END ..algorithm=kd_tree, n_neighbors=4, weights=uniform; total time=   4.1s
[CV] END ..algorithm=kd_tree, n_neighbors=4, weights=uniform; total time=   4.1s
[CV] END ..algorithm=kd_tree, n_neighbors=4, weights=uniform; total time=   4.0s
[CV] END .algorithm=kd_tree, n_neighbors=4, weights=distance; total time=   2.9s
[CV] END .algorithm=kd_tree, n_neighbors=4, weights=distance; total time=   3.1s
[CV] END ..algorithm=kd_tree, n_neighbors=5, weights=uniform; total time=   4.4s
[CV] END ..algorithm=kd_tree, n_neighbors=5, weights=uniform; total time=   4.4s
[CV] END .algorithm=kd_tree, n_neighbors=5, weights=distance; total time=   3.7s
[CV] END ..algorithm=kd_tree, n_neighbors=5, weights=uniform; total time=   4.6s
[CV] END .algorithm=kd_tree, n_neighbors=5, weights=distance; total time=   3.6s
[CV] END .algorithm=kd_tree, n_neighbors=5, weights=distance; total time=   3.6s
[CV] END ..algorithm=kd_tree, n_neighbors=7, weights=uniform; total time=   4.8s
[CV] END ..algorithm=kd_tree, n_neighbors=7, weights=uniform; total time=   4.7s
[CV] END .algorithm=kd_tree, n_neighbors=7, weights=distance; total time=   3.7s
[CV] END ..algorithm=kd_tree, n_neighbors=7, weights=uniform; total time=   4.7s
[CV] END .algorithm=kd_tree, n_neighbors=7, weights=distance; total time=   3.9s
[CV] END .algorithm=kd_tree, n_neighbors=7, weights=distance; total time=   4.0s
[CV] END ..algorithm=kd_tree, n_neighbors=9, weights=uniform; total time=   5.5s
[CV] END ..algorithm=kd_tree, n_neighbors=9, weights=uniform; total time=   5.5s
[CV] END .algorithm=kd_tree, n_neighbors=9, weights=distance; total time=   4.6s
[CV] END ..algorithm=kd_tree, n_neighbors=9, weights=uniform; total time=   5.6s
[CV] END ....algorithm=brute, n_neighbors=3, weights=uniform; total time=   1.6s
[CV] END .algorithm=kd_tree, n_neighbors=9, weights=distance; total time=   4.5s
[CV] END ....algorithm=brute, n_neighbors=3, weights=uniform; total time=   1.5s
[CV] END .algorithm=kd_tree, n_neighbors=9, weights=distance; total time=   4.6s
[CV] END ....algorithm=brute, n_neighbors=3, weights=uniform; total time=   1.5s
[CV] END ...algorithm=brute, n_neighbors=3, weights=distance; total time=   1.6s
[CV] END ...algorithm=brute, n_neighbors=3, weights=distance; total time=   1.6s
[CV] END ...algorithm=brute, n_neighbors=3, weights=distance; total time=   1.6s
[CV] END ....algorithm=brute, n_neighbors=4, weights=uniform; total time=   1.7s
[CV] END ....algorithm=brute, n_neighbors=4, weights=uniform; total time=   1.7s
[CV] END ....algorithm=brute, n_neighbors=4, weights=uniform; total time=   1.7s
[CV] END ...algorithm=brute, n_neighbors=4, weights=distance; total time=   1.7s
[CV] END ...algorithm=brute, n_neighbors=4, weights=distance; total time=   1.6s
[CV] END ...algorithm=brute, n_neighbors=4, weights=distance; total time=   1.5s
[CV] END ....algorithm=brute, n_neighbors=5, weights=uniform; total time=   1.6s
[CV] END ....algorithm=brute, n_neighbors=5, weights=uniform; total time=   1.5s
[CV] END ....algorithm=brute, n_neighbors=5, weights=uniform; total time=   1.5s
[CV] END ...algorithm=brute, n_neighbors=5, weights=distance; total time=   1.6s
[CV] END ...algorithm=brute, n_neighbors=5, weights=distance; total time=   1.6s
[CV] END ...algorithm=brute, n_neighbors=5, weights=distance; total time=   1.8s
[CV] END ....algorithm=brute, n_neighbors=7, weights=uniform; total time=   1.9s
[CV] END ....algorithm=brute, n_neighbors=7, weights=uniform; total time=   1.8s
[CV] END ....algorithm=brute, n_neighbors=7, weights=uniform; total time=   1.7s
[CV] END ...algorithm=brute, n_neighbors=7, weights=distance; total time=   1.7s
[CV] END ...algorithm=brute, n_neighbors=7, weights=distance; total time=   1.6s
[CV] END ...algorithm=brute, n_neighbors=7, weights=distance; total time=   1.6s
[CV] END ....algorithm=brute, n_neighbors=9, weights=uniform; total time=   1.7s
[CV] END ....algorithm=brute, n_neighbors=9, weights=uniform; total time=   1.6s
[CV] END ....algorithm=brute, n_neighbors=9, weights=uniform; total time=   1.6s
[CV] END ...algorithm=brute, n_neighbors=9, weights=distance; total time=   1.6s
[CV] END ...algorithm=brute, n_neighbors=9, weights=distance; total time=   1.4s
[CV] END ...algorithm=brute, n_neighbors=9, weights=distance; total time=   1.0s
---------------- Best Params for XGBoost -------------------
{'min_child_weight': 1, 'max_depth': 9}
---------------- Best Params for RF -------------------
{'n_estimators': 200, 'max_features': 'log2', 'max_depth': None}
---------------- Best Params for KNN -------------------
{'weights': 'distance', 'n_neighbors': 4, 'algorithm': 'auto'}

Retraining the Model with best Parameters

from sklearn.metrics import roc_auc_score,roc_curve
best_models = {
    "Random Forest Classifier": RandomForestClassifier(**model_param['RF']),
    "KNeighborsClassifier": KNeighborsClassifier(**model_param['KNN']),
    "XGBClassifier": XGBClassifier(**model_param['XGBoost'],n_jobs=-1),
}
tuned_report =evaluate_models(X=X_res, y=y_res, models=best_models)

Random Forest Classifier
Model performance for Training set
- Accuracy: 1.0000
- F1 score: 1.0000
- Precision: 1.0000
- Recall: 1.0000
- Roc Auc Score: 1.0000
----------------------------------
Model performance for Test set
- Accuracy: 0.9540
- F1 score: 0.9577
- Precision: 0.9548
- Recall: 0.9605
- Roc Auc Score: 0.9534
===================================


KNeighborsClassifier
Model performance for Training set
- Accuracy: 1.0000
- F1 score: 1.0000
- Precision: 1.0000
- Recall: 1.0000
- Roc Auc Score: 1.0000
----------------------------------
Model performance for Test set
- Accuracy: 0.9733
- F1 score: 0.9757
- Precision: 0.9651
- Recall: 0.9865
- Roc Auc Score: 0.9721
===================================


XGBClassifier
Model performance for Training set
- Accuracy: 0.9995
- F1 score: 0.9995
- Precision: 0.9993
- Recall: 0.9997
- Roc Auc Score: 0.9995
----------------------------------
Model performance for Test set
- Accuracy: 0.9455
- F1 score: 0.9498
- Precision: 0.9487
- Recall: 0.9508
- Roc Auc Score: 0.9450
===================================


tuned_report

	Model Name 	Accuracy
1 	KNeighborsClassifier 	0.973314
0 	Random Forest Classifier 	0.953959
2 	XGBClassifier 	0.945455

best_model = KNeighborsClassifier(**model_param['KNN'])
best_model = best_model.fit(X_train,y_train)
y_pred = best_model.predict(X_test)
score = accuracy_score(y_test,y_pred)
cr = classification_report(y_test,y_pred)

print("FINAL MODEL 'KNN'")
print ("Accuracy Score value: {:.4f}".format(score))
print (cr)

FINAL MODEL 'KNN'
Accuracy Score value: 0.9733
              precision    recall  f1-score   support

           0       0.98      0.96      0.97      1561
           1       0.97      0.99      0.98      1849

    accuracy                           0.97      3410
   macro avg       0.97      0.97      0.97      3410
weighted avg       0.97      0.97      0.97      3410

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test)

<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x109a5f7c0>

Best Model is K-Nearest Neighbor(KNN) with Accuracy 97.33%
